
import pandas as pd
import os
import gc

def processar_big_data_onchain(input_file, output_file):
    """
    Kneads Big Data into Small Data.
    L√™ arquivos gigantes (GBs) de transa√ß√µes bloco a bloco e 
    converte em um CSV di√°rio (KB) com Volume Total Agregado.
    """
    print(f"üöÄ Iniciando processamento de: {os.path.basename(input_file)}")
    print(f"üíæ Sa√≠da ser√° salva em: {output_file}")
    
    # Prepara um dicion√°rio para acumular os volumes por dia
    # { '2024-01-01': 500.2, '2024-01-02': 1020.5 ... }
    daily_volume = {}
    
    # Ler em chunks de 100.000 linhas para n√£o explodir a RAM
    chunk_size = 100000
    total_rows = 0
    
    try:
        # Tenta detectar se o arquivo tem cabe√ßalho comentado ou pula linhas iniciais
        # O arquivo do WalletExplorer tem um header chato na linha 1
        for chunk in pd.read_csv(input_file, chunksize=chunk_size, skiprows=1):
            
            # Converter coluna date
            # Formato esperado: '2025-01-28 07:26:42'
            chunk['date'] = pd.to_datetime(chunk['date']).dt.date
            
            # Garantir que amounts sejam num√©ricos
            cols_to_sum = ['received amount', 'sent amount']
            for col in cols_to_sum:
                if col in chunk.columns:
                    chunk[col] = pd.to_numeric(chunk[col], errors='coerce').fillna(0)
            
            # Agrupar este chunk por data
            # Somamos received + sent para ter o "Volume Movimentado Total"
            chunk['total_vol'] = chunk['received amount'] + chunk['sent amount']
            agg = chunk.groupby('date')['total_vol'].sum()
            
            # Adicionar ao acumulador global
            for data, vol in agg.items():
                date_str = str(data)
                daily_volume[date_str] = daily_volume.get(date_str, 0) + vol
            
            total_rows += len(chunk)
            print(f"   Processadas {total_rows} linhas...", end='\r')
            
            # Limpar mem√≥ria
            del chunk
            gc.collect()
            
    except Exception as e:
        print(f"\n‚ùå Erro cr√≠tico: {e}")
        return

    print(f"\n‚úÖ Leitura conclu√≠da! Salvando CSV agregado...")
    
    # Converter dicion√°rio para DataFrame final e salvar
    df_final = pd.DataFrame(list(daily_volume.items()), columns=['Date', 'OnChain_Volume_BTC'])
    df_final = df_final.sort_values('Date')
    df_final.to_csv(output_file, index=False)
    
    print(f"üéâ Sucesso! Arquivo gerado: {output_file}")
    print(df_final.head())

if __name__ == "__main__":
    # Exemplo de uso: Rodar apenas se chamado diretamente
    # Caminhos relativos assumindo execu√ß√£o da raiz
    data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'data')
    
    # Lista de arquivos para processar (Exemplo: Huobi que √© menor p/ teste local)
    target = "walletexplorer-Huobi_com-000012a55e988d91.csv"
    input_path = os.path.join(data_dir, target)
    output_path = os.path.join(data_dir, "processed_huobi_daily.csv")
    
    if os.path.exists(input_path):
        processar_big_data_onchain(input_path, output_path)
    else:
        print(f"Arquivo n√£o encontrado: {input_path}")
